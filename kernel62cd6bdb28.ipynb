{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train= pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test= pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(\"location\", inplace=True, axis=1)\ndf_test.drop(\"location\", inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets start with lowercasing, droping numebrs, removing punctuation and white spaces from both datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nfor i in range(df_train[\"text\"].size):\n    df_train[\"text\"][i]=df_train[\"text\"][i].lower()\n    df_train[\"text\"][i]=re.sub(r'\\d+', \"\", df_train[\"text\"][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(df_train[\"text\"].size):\n    result=df_train[\"text\"][i].translate(str.maketrans('','', string.punctuation))\n    df_train[\"text\"][i]=result\n    df_train[\"text\"][i]=df_train[\"text\"][i].strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nfor i in range(df_test[\"text\"].size):\n    df_test[\"text\"][i]=df_test[\"text\"][i].lower()\n    df_test[\"text\"][i]=re.sub(r'\\d+', \"\", df_test[\"text\"][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(df_test[\"text\"].size):\n    result=df_test[\"text\"][i].translate(str.maketrans('','', string.punctuation))\n    df_test[\"text\"][i]=result\n    df_test[\"text\"][i]=df_test[\"text\"][i].strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets begin with tokenizing and removing stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nstemmer = PorterStemmer()\nfor i in range(df_train[\"text\"].size):\n    traintokenz=word_tokenize(df_train[\"text\"][i])     #tokenizing each tweet\n    for word in traintokenz:\n        if word in stopwords.words('english'):       #removing stopwords from tokens\n            traintokenz.remove(word)\n    for i in range(len(traintokenz)):\n            traintokenz[i] = lemmatizer.lemmatize(traintokenz[i])\n            traintokenz[i] = stemmer.stem(traintokenz[i])    # stemming the tokens\n    df_train[\"text\"][i]=\" \".join(traintokenz)      #back to the dataframe, the converted string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\nfor i in range(df_test[\"text\"].size):\n    traintokenz=word_tokenize(df_test[\"text\"][i])    #tokenizing each tweet\n    for word in traintokenz:\n        if word in stopwords.words('english'):       # removing stopwords from tokens\n            traintokenz.remove(word)\n    for i in range(len(traintokenz)):\n            traintokenz[i] = lemmatizer.lemmatize(traintokenz[i])  #lemmatizing\n            traintokenz[i] = stemmer.stem(traintokenz[i])     #stemming tokens\n            \n    df_test[\"text\"][i]=\" \".join(traintokenz)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fill the nan values in keywords and try to one-hot encode the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"values={\"id\",\"keyword\"}\ntrain_keywords=df_train[values]\ntest_keywords=df_test[values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_keywords[\"keyword\"].fillna(value=train_keywords[\"keyword\"].mode(1),inplace=True)\ntest_keywords[\"keyword\"].fillna(value=train_keywords[\"keyword\"].mode(1),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_keywords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_keywords=pd.get_dummies(train_keywords,columns = [\"keyword\"])\ntest_keywords=pd.get_dummies(test_keywords,columns=[\"keyword\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_1=df_train.drop(\"keyword\",axis=1,inplace=False)\ndf_test_1=df_test.drop(\"keyword\",axis=1,inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=df_train_1.drop(\"target\",axis=1,inplace=False)\ny_train=df_train_1[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CountVectorizer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nmatrix = CountVectorizer(max_features=12000)\nXtrain = matrix.fit_transform(x_train[\"text\"]).toarray()\nXtest = matrix.transform(df_test_1[\"text\"]).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2train=pd.concat([train_keywords, pd.DataFrame(Xtrain)], axis=1)\ndf2test=pd.concat([test_keywords, pd.DataFrame(Xtest)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new,X_test_new=TFIDF(x_train[\"text\"],df_test_1[\"text\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train-tes split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nx_new_train, x_new_test, y_new_train, y_new_test = train_test_split(df2train, y_train, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nx_new_train, x_new_test, y_new_train, y_new_test = train_test_split(Xtrain, y_train, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deploying a simple ReLU, Softmax neural net"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_new_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import  Dropout, Dense\nfrom keras.models import Sequential\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):\n    \"\"\"\n    buildModel_DNN_Tex(shape, nClasses,dropout)\n    Build Deep neural networks Model for text classification\n    Shape is input feature space\n    nClasses is number of classes\n    \"\"\"\n    model = Sequential()\n    node = 12000 # number of nodes\n    nLayers = 4 # number of  hidden layer\n    model.add(Dense(node,input_dim=shape,activation='relu'))\n    model.add(Dropout(dropout))\n    for i in range(0,nLayers):\n        model.add(Dense(node,input_dim=node,activation='relu'))\n        model.add(Dropout(dropout))\n    model.add(Dense(nClasses, activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_DNN = Build_Model_DNN_Text(x_new_train.shape[1], 20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_DNN.fit(x_new_train,y_new_train, validation_data=(x_new_test,y_new_test), epochs=10,batch_size=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes \nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_new_train, y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_new_test)\nprint(metrics.classification_report(y_new_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_new_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GradientBoosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nmodel=GradientBoostingClassifier(n_estimators=20)\nmodel.fit(x_new_train, y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(x_new_test)\nprint(metrics.classification_report(y_new_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel2=BaggingClassifier()\nmodel2.fit(x_new_train,y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y_pred = model2.predict(x_new_test)\nprint(metrics.classification_report(y_new_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nmodel3= LinearSVC(C=0.1, loss='hinge',max_iter=20000)\nmodel3.fit(x_new_train,y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model3.predict(x_new_test)\nprint(metrics.classification_report(y_new_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nmodel4=tree.DecisionTreeClassifier()\nmodel4.fit(x_new_train,y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model4.predict(x_new_test)\nprint(metrics.classification_report(y_new_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_new_test)\nprint(metrics.classification_report(y_new_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_2_pred=model.predict(x_new_test)\nprint(metrics.classification_report(y_new_test, y_2_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Conclusion***\n\n\nAfter evaluating each model on the train-test split dataset, and fine tuning some regularization terms, we can argue that the best model among these is the \"Support Vector Classifier\" and now we predict the test data and submit it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nmodel3= LinearSVC(C=0.2,loss='hinge')\nmodel3.fit(Xtrain,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model3.predict(Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={'id':df_test_1[\"id\"] ,'target':y_pred}\nMub=pd.DataFrame(d)\nMub.to_csv('/kaggle/working/Mub.csv',index=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}